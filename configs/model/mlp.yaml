# MLP Model Configuration

# Hidden layer dimensions
hidden_dims: [64, 128, 256, 128, 64]

# Activation function
activation: "relu"  # Options: relu, leaky_relu, elu, gelu

# Dropout
dropout: 0.1

# Whether to use batch normalization
use_batch_norm: true
