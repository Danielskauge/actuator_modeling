/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/daniel/workspace/actuator_modeling/models/checkpoints/None exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/daniel/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

Actuator Model Summary:
Input dimension: 20
Hidden dimensions: [64, 128, 256, 128, 64]
Total parameters: 85185
Trainable parameters: 85185

  | Name  | Type | Params | Mode
---------------------------------------
0 | model | MLP  | 85.2 K | train
---------------------------------------
85.2 K    Trainable params
0         Non-trainable params
85.2 K    Total params
0.341     Total estimated model params size (MB)
15        Modules in train mode
0         Modules in eval mode


Epoch 19: 100%|█| 47/47 [00:01<00:00, 32.22it/s, v_num=3s19, train_loss_step=4.020, val_loss=6.360,TorchScript export failed:
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
'numpy.int64' object in attribute 'Linear.in_features' is not a valid constant.                    
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
Valid constants are:
1. a nn.ModuleList
2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}
3. a list or tuple of (2)

Saving the model state dict instead...
Model state dict saved to models/checkpoints/../exported/model_state_dict.pt
/home/daniel/workspace/actuator_modeling/src/models/mlp.py:107: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if residual is not None and residual.shape[1] == x.shape[1]:
ONNX model saved to models/checkpoints/../exported/model.onnx
Epoch 19: 100%|█| 47/47 [00:01<00:00, 28.91it/s, v_num=3s19, train_loss_step=4.020, val_loss=6.360,
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
Testing DataLoader 0: 100%|███████████████████████████████████████| 16/16 [00:00<00:00, 118.54it/s]
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m [0m[1m       Test metric       [0m[1m [0m┃[1m [0m[1m      DataLoader 0       [0m[1m [0m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m [0m[36m        test_loss        [0m[36m [0m│[35m [0m[35m    6.333183288574219    [0m[35m [0m│
└───────────────────────────┴───────────────────────────┘
