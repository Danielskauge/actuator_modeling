/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/daniel/workspace/actuator_modeling/models/checkpoints/None exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/daniel/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

Actuator Model Summary:
Input dimension: 20
Hidden dimensions: [64, 128, 256, 128, 64]
Total parameters: 85185
Trainable parameters: 85185

  | Name  | Type | Params | Mode
---------------------------------------
0 | model | MLP  | 85.2 K | train
---------------------------------------
85.2 K    Trainable params
0         Non-trainable params
85.2 K    Total params
0.341     Total estimated model params size (MB)
15        Modules in train mode
0         Modules in eval mode


Epoch 19: 100%|█| 47/47 [00:01<00:00, 31.32it/s, v_num=i0s1, train_loss_step=4.020, val_loss=6.360, train_losTorchScript model saved to models/checkpoints/../exported/model.pt
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
ONNX model saved to models/checkpoints/../exported/model.onnx                                                
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/daniel/workspace/actuator_modeling/src/models/mlp.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if residual is not None and residual.shape[1] == x.shape[1]:
Epoch 19: 100%|█| 47/47 [00:01<00:00, 26.67it/s, v_num=i0s1, train_loss_step=4.020, val_loss=6.360, train_los
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
Testing DataLoader 0: 100%|█████████████████████████████████████████████████| 16/16 [00:00<00:00, 124.57it/s]
/home/daniel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m [0m[1m       Test metric       [0m[1m [0m┃[1m [0m[1m      DataLoader 0       [0m[1m [0m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m [0m[36m        test_loss        [0m[36m [0m│[35m [0m[35m    6.333183288574219    [0m[35m [0m│
└───────────────────────────┴───────────────────────────┘
